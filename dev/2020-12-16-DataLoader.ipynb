{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"image_names.txt\", \"w\")\n",
    "image_names = os.listdir('../data/ALLSTIMULI/')[2:-3]\n",
    "for i in range(len(image_names)):\n",
    "    names.write(image_names[i][:-5]+'\\n')\n",
    "names.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"image_names.txt\", \"r\")\n",
    "img_names = names.readlines()\n",
    "for i in range(len(img_names)):\n",
    "    img_names[i]=img_names[i][:-1]\n",
    "    \n",
    "loc_data_xy={}\n",
    "for name in img_names:\n",
    "    locpath = '../data/loc_data/' + name\n",
    "    f = open(locpath,'rb')\n",
    "    loc_dict = pickle.load(f)\n",
    "    loc_data_xy[name] = np.array(loc_dict['barycenters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "    #plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaccadeLandmarksDataset(Dataset):\n",
    "    \"\"\"Saccade Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, loc_dict, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loc_dir (string): Path to the saccade location file\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.loc_dict = loc_dict\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loc_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = os.listdir(self.img_dir)[idx+2]\n",
    "        img_path = os.path.join(self.img_dir,img_name)\n",
    "        image = io.imread(img_path)\n",
    "        name = img_name[:-5]\n",
    "        landmarks = self.loc_dict[name]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.reshape(-1, 2) #.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSaccadeTo(object):\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        nb_sac = len(landmarks)\n",
    "        sac_num =  np.random.randint(nb_sac)\n",
    "        sac = landmarks[sac_num]\n",
    "        N_X, N_Y = image.shape[:2]\n",
    "        #img_color_sac = saccade_to(image, (N_X//2, N_Y//2), (sac[1], sac[0]))\n",
    "        image_roll = np.copy(image)\n",
    "        image_roll=np.roll(image_roll, N_X//2 - sac[1], axis=0)\n",
    "        image_roll=np.roll(image_roll, N_Y//2 - sac[0], axis=1)\n",
    "        return {'image':image_roll, 'pos':sac}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image_tens = sample['image'].transpose((2, 0, 1))\n",
    "        return {'image': torch.FloatTensor(image_tens), 'pos': sample['pos']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted cropped pyramid (squeezed tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lg shape= 32 32\n"
     ]
    }
   ],
   "source": [
    "from PYramid import cropped_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppedPyramid(object):\n",
    "    def __init__(self, width, base_levels, color=True, do_mask=False, verbose=False):\n",
    "        self.width = width\n",
    "        self.base_levels = base_levels\n",
    "        self.color = color\n",
    "        self.do_mask = do_mask\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img_crop, level_size = cropped_pyramid(sample['image'].unsqueeze(0), \n",
    "                                               width=self.width, \n",
    "                                               base_levels=self.base_levels,\n",
    "                                               color=self.color, \n",
    "                                               do_mask=self.do_mask, \n",
    "                                               verbose=self.verbose,\n",
    "                                               squeeze=True)\n",
    "        return{'img_crop':img_crop, 'level_size':level_size, 'pos':sample['pos']}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogGaborTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=32\n",
    "base_levels=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LogGabor import LogGabor\n",
    "from PYramid import local_filter\n",
    "from PYramid import get_K\n",
    "from PYramid import log_gabor_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sublevel = 2 \n",
    "n_azimuth = 12 \n",
    "n_theta = 12\n",
    "n_phase = 2\n",
    "\n",
    "pe = {'N_X': width, 'N_Y': width, 'do_mask': False, 'base_levels':\n",
    "          base_levels, 'n_theta': 24, 'B_sf': 0.6, 'B_theta': np.pi/12 ,\n",
    "      'use_cache': True, 'figpath': 'results', 'edgefigpath':\n",
    "          'results/edges', 'matpath': 'cache_dir', 'edgematpath':\n",
    "          'cache_dir/edges', 'datapath': 'database/', 'ext': '.pdf', 'figsize':\n",
    "          14.0, 'formats': ['pdf', 'png', 'jpg'], 'dpi': 450, 'verbose': 0}   \n",
    "\n",
    "lg = LogGabor(pe)\n",
    "K = get_K(width=width,\n",
    "            n_sublevel = n_sublevel, \n",
    "          n_azimuth = n_azimuth, \n",
    "                  n_theta = n_theta,\n",
    "                  n_phase = n_phase, \n",
    "                  r_min = width/6, \n",
    "                  r_max = width/3, \n",
    "                  log_density_ratio = 2, \n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogGaborTransform(object):\n",
    "    def __init__(self, K=K, color=True, verbose=False):\n",
    "        self.K = K\n",
    "        self.color = color\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        log_gabor_coeffs = log_gabor_transform(sample['img_crop'].unsqueeze(0), K)\n",
    "        \n",
    "        return{'img_gabor':log_gabor_coeffs, 'K':K}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplexModulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose transforms\n",
    "### transforms.Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show a batch\n",
    "def show_landmarks_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    for level in range(5,0,-1):\n",
    "        plt.figure()\n",
    "        images_batch = sample_batched['img_crop'][:,level,:,:,:]\n",
    "        batch_size = len(images_batch)\n",
    "        im_size = images_batch.size(2)\n",
    "        grid_border_size = 2\n",
    "\n",
    "        grid = utils.make_grid(images_batch)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)).clip(0,255).astype('uint8'))\n",
    "\n",
    "        plt.title('Batch from dataloader, level=' + str(level))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_X = width\n",
    "N_Y = width\n",
    "n_sublevel = 2 \n",
    "n_azimuth = 12 \n",
    "n_theta = 12\n",
    "n_phase = 2\n",
    "n_levels = int(np.log(np.max((N_X, N_Y))/width)/np.log(base_levels)) + 1\n",
    "n_eccentricity = 2\n",
    "n_color = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, in_chan = n_levels * n_color * n_eccentricity * n_azimuth * n_theta * n_phase, out_chan = 100):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(in_chan=in_chan, out_chan=out_chan)\n",
    "        self.decoder = Decoder(in_chan=out_chan, out_chan=in_chan)\n",
    "\n",
    "    def forward(self, x, **kargs):\n",
    "        code = self.encoder(x)\n",
    "        out = self.decoder(code)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc = nn.Linear(in_chan, out_chan)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chan, out_chan):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec = nn.Linear(in_chan, out_chan)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.dec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1728, out_features=100, bias=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder.enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0048,  0.0118, -0.0006,  ..., -0.0165, -0.0185, -0.0056],\n",
       "        [ 0.0006, -0.0073, -0.0144,  ...,  0.0162, -0.0165,  0.0097],\n",
       "        [ 0.0227, -0.0181,  0.0221,  ...,  0.0145,  0.0169,  0.0100],\n",
       "        ...,\n",
       "        [-0.0101,  0.0035,  0.0027,  ...,  0.0071,  0.0223, -0.0039],\n",
       "        [-0.0167, -0.0140,  0.0129,  ..., -0.0100,  0.0236,  0.0061],\n",
       "        [-0.0168, -0.0221,  0.0037,  ..., -0.0078, -0.0206,  0.0011]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.encoder.enc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=100, out_features=1728, bias=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoenc.decoder.dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(autoenc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saccade_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0114ad4682d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m dataloader = DataLoader(saccade_dataset, batch_size=batch_size,\n\u001b[0m\u001b[0;32m      2\u001b[0m                         shuffle=True, num_workers=0)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'saccade_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(saccade_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_batched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-1d579e29339b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_batched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'img_crop'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_batched' is not defined"
     ]
    }
   ],
   "source": [
    "sample_batched['img_crop'] #input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
