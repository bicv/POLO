{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"image_names.txt\", \"w\")\n",
    "image_names = os.listdir('../data/ALLSTIMULI/')[2:-3]\n",
    "for i in range(len(image_names)):\n",
    "    names.write(image_names[i][:-5]+'\\n')\n",
    "names.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open(\"image_names.txt\", \"r\")\n",
    "img_names = names.readlines()\n",
    "for i in range(len(img_names)):\n",
    "    img_names[i]=img_names[i][:-1]\n",
    "    \n",
    "loc_data_xy={}\n",
    "for name in img_names:\n",
    "    locpath = '../data/loc_data/' + name\n",
    "    f = open(locpath,'rb')\n",
    "    loc_dict = pickle.load(f)\n",
    "    loc_data_xy[name] = np.array(loc_dict['barycenters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loc_data_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "    #plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ALLSTIMULI/i05june05_static_street_boston_p1010764\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ALLSTIMULI/i05june05_static_street_boston_p1010764'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b99c09a08c17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mimpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../data/ALLSTIMULI/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mimg_npy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc_data_xy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2877\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2878\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2879\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ALLSTIMULI/i05june05_static_street_boston_p1010764'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name in img_names[:30]:\n",
    "    plt.figure()\n",
    "    impath = '../data/ALLSTIMULI/' + name + ''\n",
    "    print(impath)\n",
    "    img = Image.open(impath)\n",
    "    img_npy = np.asarray(img)\n",
    "    landmarks = np.array(loc_data_xy[name])\n",
    "    show_landmarks(img_npy, landmarks)\n",
    "    #loc_ij = []\n",
    "    #for coord in loc_xy:\n",
    "    #    loc_ij += [[coord[1], coord[0]]]\n",
    "    #loc_ij = np.array(loc_ij)\n",
    "    #plt.plot(loc_data_ij_part[:,1], loc_data_ij_part[:,0])\n",
    "    #for i, coord in enumerate(loc_xy):\n",
    "    #    plt.plot(coord[0], coord[1], 'r+', ms=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaccadeLandmarksDataset(Dataset):\n",
    "    \"\"\"Saccade Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, loc_dict, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loc_dir (string): Path to the saccade location file\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.loc_dict = loc_dict\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loc_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = os.listdir(self.img_dir)[idx+2]\n",
    "        img_path = os.path.join(self.img_dir,img_name)\n",
    "        image = io.imread(img_path)\n",
    "        name = img_name[:-5]\n",
    "        landmarks = self.loc_dict[name]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.reshape(-1, 2) #.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saccade_dataset = SaccadeLandmarksDataset(loc_dict=loc_data_xy,\n",
    "                                    img_dir='../data/ALLSTIMULI/')\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(saccade_dataset)):\n",
    "    sample = saccade_dataset[i]\n",
    "\n",
    "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_landmarks(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample['landmarks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSaccadeTo(object):\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        nb_sac = len(landmarks)\n",
    "        sac_num =  np.random.randint(nb_sac)\n",
    "        sac = landmarks[sac_num]\n",
    "        N_X, N_Y = image.shape[:2]\n",
    "        #img_color_sac = saccade_to(image, (N_X//2, N_Y//2), (sac[1], sac[0]))\n",
    "        image_roll = np.copy(image)\n",
    "        image_roll=np.roll(image_roll, N_X//2 - sac[1], axis=0)\n",
    "        image_roll=np.roll(image_roll, N_Y//2 - sac[0], axis=1)\n",
    "        return {'image':image_roll, 'pos':sac}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sac = RandomSaccadeTo()(sample)\n",
    "plt.imshow(sample_sac['image'])\n",
    "N_X, N_Y = sample_sac['image'].shape[:2]\n",
    "plt.scatter(N_Y//2, N_X//2, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image_tens = sample['image'].transpose((2, 0, 1))\n",
    "        return {'image': torch.FloatTensor(image_tens), 'pos': sample['pos']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tens = ToTensor()(sample_sac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted cropped pyramid (squeezed tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PYramid import cropped_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroppedPyramid(object):\n",
    "    def __init__(self, width, base_levels, color=True, do_mask=False, verbose=False):\n",
    "        self.width = width\n",
    "        self.base_levels = base_levels\n",
    "        self.color = color\n",
    "        self.do_mask = do_mask\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        img_crop, level_size = cropped_pyramid(sample['image'].unsqueeze(0), \n",
    "                                               width=self.width, \n",
    "                                               base_levels=self.base_levels,\n",
    "                                               color=self.color, \n",
    "                                               do_mask=self.do_mask, \n",
    "                                               verbose=self.verbose,\n",
    "                                               squeeze=True)\n",
    "        return{'img_crop':img_crop, 'level_size':level_size, 'pos':sample['pos']}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=32\n",
    "base_levels=2\n",
    "cropped_pyr_transform = CroppedPyramid(width, base_levels)\n",
    "transformed_data = cropped_pyr_transform(sample_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data['img_crop'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LogGabor import LogGabor\n",
    "from PYramid import local_filter\n",
    "from PYramid import get_K\n",
    "from PYramid import log_gabor_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sublevel = 2 \n",
    "n_azimuth = 12 \n",
    "n_theta = 12\n",
    "n_phase = 2\n",
    "\n",
    "pe = {'N_X': width, 'N_Y': width, 'do_mask': False, 'base_levels':\n",
    "          base_levels, 'n_theta': 24, 'B_sf': 0.6, 'B_theta': np.pi/12 ,\n",
    "      'use_cache': True, 'figpath': 'results', 'edgefigpath':\n",
    "          'results/edges', 'matpath': 'cache_dir', 'edgematpath':\n",
    "          'cache_dir/edges', 'datapath': 'database/', 'ext': '.pdf', 'figsize':\n",
    "          14.0, 'formats': ['pdf', 'png', 'jpg'], 'dpi': 450, 'verbose': 0}   \n",
    "\n",
    "lg = LogGabor(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = get_K(width=width,\n",
    "            n_sublevel = n_sublevel, \n",
    "          n_azimuth = n_azimuth, \n",
    "                  n_theta = n_theta,\n",
    "                  n_phase = n_phase, \n",
    "                  r_min = width/6, \n",
    "                  r_max = width/3, \n",
    "                  log_density_ratio = 2, \n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogGaborTransform(object):\n",
    "    def __init__(self, K=K, color=True, verbose=False):\n",
    "        self.K = K\n",
    "        self.color = color\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        log_gabor_coeffs = log_gabor_transform(sample['img_crop'].unsqueeze(0), K)\n",
    "        \n",
    "        return{'img_gabor':log_gabor_coeffs, 'K':K}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform = LogGaborTransform()\n",
    "transformed_data = my_transform(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([RandomSaccadeTo(),\n",
    "                               ToTensor(),\n",
    "                               CroppedPyramid(width, base_levels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = composed_transform(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saccade_dataset = SaccadeLandmarksDataset(loc_dict=loc_data_xy,\n",
    "                                          img_dir='../data/ALLSTIMULI/',\n",
    "                                          transform=composed_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(saccade_dataset)):\n",
    "    sample = saccade_dataset[i]\n",
    "\n",
    "    print(i, sample['img_crop'].size(), sample['level_size'])\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to show a batch\n",
    "def show_landmarks_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    for level in range(5,0,-1):\n",
    "        plt.figure()\n",
    "        images_batch = sample_batched['img_crop'][:,level,:,:,:]\n",
    "        batch_size = len(images_batch)\n",
    "        im_size = images_batch.size(2)\n",
    "        grid_border_size = 2\n",
    "\n",
    "        grid = utils.make_grid(images_batch)\n",
    "        plt.imshow(grid.numpy().transpose((1, 2, 0)).clip(0,255).astype('uint8'))\n",
    "\n",
    "        plt.title('Batch from dataloader, level=' + str(level))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(saccade_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['img_crop'].size())\n",
    "    if i_batch ==3 :\n",
    "        plt.figure()\n",
    "        show_landmarks_batch(sample_batched)    \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
